[
  {
    "slug": "affine-augmentations",
    "title": "Affine augmentations",
    "excerpt": "Old notes on affine augmentations. Enjoy!",
    "content": " Old notes on affine augmentations. Enjoy!             Affine Augmentations            Most of the standard Geometric 2D augmentations can be composed from           affine matrices.          Lets load an image with bbox and keypoint labels  ```python import cv2 import yaml from PIL import Image  Read image img = cv2.imread('./augmentation_data.png') print(f\"Image size = width:{img.shape[1]} pixels / height:{img.shape[0]} pixels\")  Read labels labels = yaml.safe_load(open('./augmentation_data_labels.yml'))['labels'] print(labels) ```              {`Image size = width:599 pixels / height:464 pixels {'wickets': {'x1': 253, 'y1': 250, 'x2': 315, 'y2': 434}, 'bat': {'x1': 372, 'y1': 18, 'x2': 482, 'y2': 69}, 'wicket_camera': {'x': 288, 'y': 386}, 'r_eye': {'x': 314, 'y': 108}, 'l_eye': {'x': 329, 'y': 103}}`}          Plot labels  ```python #Wickets image = cv2.rectangle(img, (labels['wickets']['x1'], labels['wickets']['y1']),\\     (labels['wickets']['x2'], labels['wickets']['y2']), (0, 0 , 255), 2) #Bat image = cv2.rectangle(img, (labels['bat']['x1'], labels['bat']['y1']),\\     (labels['bat']['x2'], labels['bat']['y2']), (0, 255 , 255), 2) Wicket camera image = cv2.circle(img, (labels['wicket_camera']['x'], labels['wicket_camera']['y']), \\     2, (0, 255, 0), 1) l_eye camera image = cv2.circle(img, (labels['l_eye']['x'], labels['l_eye']['y']), \\     2, (0, 255, 0), 1) r_eye camera image = cv2.circle(img, (labels['r_eye']['x'], labels['r_eye']['y']), \\     2, (0, 255, 0), 1)  pil_image = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) display(pil_image) ```             Affine Matrix Calculator class            This class composes a sequence of rotation, translation, scaling and           shearing into one matrix.  ```python from typing import Any import numpy as np  class AffineMatrixCalculator:      def __init__(self):         pass      def __call__(self, rotation=0., translation=(0., 0.) ,scale=(1., 1.), shear=(0., 0.)):         self.M_R = self.getRotationMatrix(rotation)         self.M_t = self.getTranslationMatrix(translation)         self.M_Sc = self.getScalingMatrix(scale)         self.M_Sh = self.getShearingMatrix(shear)         return self.M_t @ self.M_R @ self.M_Sc @ self.M_Sh      @staticmethod     def getRotationMatrix(deg):         angle = np.radians(deg)         R=np.array([[np.cos(angle), -np.sin(angle), 0],\\\\                     [np.sin(angle), np.cos(angle), 0],\\\\                     [0, 0, 1]])         return R      @staticmethod     def getTranslationMatrix(trans):         M = np.eye(3)         M[0, 2] = trans[0]         M[1, 2] = trans[1]         return M      @staticmethod     def getScalingMatrix(s):         M = np.eye(3)         M[0, 0] = s[0]         M[1, 1] = s[1]         return M      @staticmethod     def getShearingMatrix(s):         M = np.eye(3)         M[0, 1] = s[0]         M[1, 0] = s[1]         return M  A = AffineMatrixCalculator() M = A(rotation=10., translation=(0.1, 0.2), scale=(0.7, 0.9), shear=(0.02, 0.05)) print(M) ```              {`[[ 0.68155126 -0.14249605  0.1       ]  [ 0.16587007  0.88875805  0.2       ]  [ 0.          0.          1.        ]]`}          Warping the image with the M matrix  ```python aug_img = cv2.warpAffine(img, M[:2, :], (img.shape[1],img.shape[0])) pil_image = Image.fromarray(cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)) display(pil_image) ```          Warping the labels  ```python aug_labels = {} w = int(abs(labels['wickets']['x1'] - labels['wickets']['x2'])) h = int(abs(labels['wickets']['y1'] - labels['wickets']['y2'])) wickets = np.array([[labels['wickets']['x1'], labels['wickets']['y1'], 1.], \\\\                     [labels['wickets']['x1']+w, labels['wickets']['y1'], 1.], \\\\                     [labels['wickets']['x1']+w, labels['wickets']['y1']+h, 1.], \\\\                     [labels['wickets']['x1'], labels['wickets']['y1']+h, 1.]])  transforming points augmented_wickets = (M @ wickets.T).T  plotting aug_img = cv2.polylines(aug_img, pts=np.int32([augmented_wickets[:, :2]]), isClosed=True, color=(0, 0, 255), thickness=2) pil_image = Image.fromarray(cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)) display(pil_image) ```"
  },
  {
    "slug": "can-llms-reason",
    "title": "NOTES: Can LLMs truly reason?",
    "excerpt": "",
    "content": " Can LLMs actually reason, or are they just “probabilistic pattern           matchers” ? This paper (              GSM-Symbolic: Understanding the Limitations of Mathematical             Reasoning in Large Language Models. by Mirzadeh et al.            ) attempts to answer that question.             Motivation:              GSM8K benchmark is widely used to assess the mathematical reasoning             of models on grade-school-level questions.              While the performance of LLMs on GSM8K has significantly improved             in recent years, it remains unclear whether their mathematical             reasoning capabilities have genuinely advanced, raising questions             about the reliability of the reported metrics.             Datasets:                GSM-Symbolic:             {' '}             an enhanced benchmark that generates diverse variants of GSM8K             questions using symbolic templates                GSM-No Op              : To create the templates, we add seemingly relevant but ultimately             inconsequential statements to GSM-Symbolic templates. Since these             statements carry no operational significance, we refer to them as             \"No-Op\". These additions do not affect the reasoning required to             solve the problem.              GSM-Symbolic-{'{M1, P1, P2}'} : GSM-Symbolic with             increasing difficulty M1 {'             Findings:              LLMs exhibit noticeable variance when responding to different             instantiations of the same question. Specifically, the performance             of all models declines when only the numerical values in the             question are altered in the GSM-Symbolic benchmark.              When we add a single clause that appears relevant to the question,             we observe significant performance drops (up to 65%) across all             state-of-the-art models, even though the added clause does not             contribute to the reasoning chain needed to reach the final             answer.              LLMs exhibit more robustness to changes in superficial elements             like proper names but are very sensitive to changes in numerical             values              Results on GSM8K demonstrate that the performance of LLMs can be             viewed as a distribution with unwarranted variance across different             instantiations of the same question.              By adding seemingly relevant but ultimately irrelevant information             to problems, we demonstrate substantial performance drops (up to             65%) across all state-of-the-art models              current LLMs are not capable of genuine logical reasoning; instead,             they attempt to replicate the reasoning steps observed in their             training data.             Techniques:              Varying numbers, names or both in the GSM-Symbolic dataset to study             the variance in responses.              Adding additional constraints to the question, relevant or             irrelevant, and studying the responses.              Measuring the decrease in accuracy as the complexity of questions             rises             Results:              Performance drop on GSM-Symbolic VS GSM8K. Most likely these             models have memorized the results on GSM8K, since it has been             publicly available for a long time              Models are more robust to variations in Name, but less so for             numbers.              As the difficulty of questions increases, model performance reduces             and variance in responses rises. This loosely suggests that the             model can’t think logically.              Models blindly infer certain operations, proving that they have             memorized responses. In Both responses, models subtract the kiwis             without being asked to do so."
  },
  {
    "slug": "cpu-vs-gpu-vs-tpu",
    "title": "CPU vs GPU vs TPU",
    "excerpt": "",
    "content": "              Introduction              While most deep learning engineers/enthusiasts focus on algorithms,             they often forget about the hardware they use for training/inference.             If you ask them why GPU/TPU is faster than a CPU, you'll often hear             responses like \"GPUs are optimized for convolutions or GPUs can run             more threads\". While these statements are true, they merely scratch             the surface. In this post, we dig deeper into the hardware to             explain what's happening.              I'm not a hardware pro by any means, but I feel this information is             critical for all AI enthusiasts."
  },
  {
    "slug": "designing-and-shipping-a-ml-feature",
    "title": "Designing and shipping a ML Feature",
    "excerpt": "",
    "content": " What exactly are we trying to accomplish? Will the new model           architecture really be a game-changer? How much impact will this new           dataset have on the model accuracy?            These are some questions a typical machine learning team deals with on           a daily basis.            But it is important to not get bogged down by the intricacies, and           always go back to the first principles approach. With the pace at           which new research papers are pouring in, it is important for           ML/Data/CV Scientists to sometimes take a back seat and think about           the overall picture a bit more.            If we start off with a tiny open-source dataset and loop through           this design cycle, we can establish a pretty solid baseline. The bulk           of the work after this should be focused on the finer aspects of the           product, the outliers/corner cases in particular.             Establishing goals and use-cases            This is perhaps the most important part of this strategy. It involves           working closely with the product managers to decide the overarching           goal, which dictates the key requirements of the product. Any mistake           here can prove to be very costly, both from employee retention and a           financial standpoint. After all, no one likes to work for managers           who keep shifting the goal post.            Here are some key components of each strategy. I refrained from going           into details for each component here, as it's mostly           self-explanatory. Instead, I wanted to focus more on the vast breadth           of issues here, since the finer details vary for each team.             Data Strategy              Data Collection → Using \"off the shelf\" datasets or             creating a dataset from scratch?              Data Labeling → Manual or Automatic annotation?              Data Backend → Storage, Indexing, and Delivery.             Usually, AWS S3 is used for storage, while teams tend to build their             own layer (typically python-based) for serving the data.             Model Strategy              Training Framework → Pytorch or Tensorflow?              Training Infrastructure → AWS or self-built GPU rigs             or both?              \"Off the shelf\" models for establishing baseline             (if available)              Exploring the Accuracy / Speed TradeOff → This blog             details how EfficientNet does exactly that.              Neural architecture search → This blog talks about             how NAS can be used in specific cases to greatly reduce the network             size while maintaining accuracy             Deployment Strategy              On the cloud → Typically web apps are hosted on AWS,             and the model will run in a multi-threaded manner on multiple GPU             instances.              On embedded device → Mobile phones, ASICS/FPGA, or             other chipsets. While iPhones offer a painless experience in             deploying ML models, android and other devices usually require more             custom solutions, although that has started to change now.              Model compatibility → This issue usually arises             while deploying on embedded devices. Some ML ops may not be             supported by the hardware, thus influencing the model strategy to             accommodate the changes.              Code consistency → This issue usually arises while             deploying on embedded devices. If there's additional logic wrapping             the model, it needs to be tested offline and online to ensure             consistency.              Speed Feedback → Quick prototyping may reveal some             flaws in the model design causing latency issues. This influences             the model strategy.             Quality Testing Strategy              Right Metrics → Going back to the goal establishment             phase, this is where we need to make sure the performance metrics             mirror the goals.              True fine-grained analysis → Qualitative and             Quantitative analysis of the model is essential for iterative             improvement. This impacts every strategy.            This is not an exhaustive list by any means, but many ML teams           usually follow this model. Feel free to leave comments."
  },
  {
    "slug": "detection-classification-metrics",
    "title": "Detection/Classification metrics",
    "excerpt": "",
    "content": "            Problem Statement              Once we've trained multiple detection/classification models, how to             choose the best model ?              Once we've chosen the best model, how to choose the optimum             operating point (the best threshold) ?             Solution            There are two metrics which can help us here. If you've come across           terms like AP, mAP and F-1 score in research papers, these are           precisely the metrics that help us with the above mentioned           problems. Let's begin by defining precision and recall, which are           pre-requisities to understanding other metrics.            Precision and Recall          Let's assume that we've trained a car detector.            Precision is the ratio of true positives to total number of           predictions. For every N predictions made by this detector on an           image, this metric tells us what percentage of those detections are           actually cars.          {\"$$ Precision = \\\\frac{TP}{TP+FP} $$\"}            Recall is the ratio of correct predictions to the number of ground           truth labels available for the class. Assuming that we have X cars in           an image, this metric tells us what percentage of those X cars were           detected correctly.          {\"$$ Recall = \\\\frac{TP}{TP+FN} $$\"}            Average Precision (AP)            Average Precision is the area under the Precision - Recall curve.          {\"$$ AP = \\\\int_0^1 P(r) ,dr $$\"}            We can use the standard sklearn package to compute the AUC (area under           the curve), or we can approximate the area, as shown in the figure           above.            Mean Average Precision (mAP) is the mean of AP for all classes.          {\"$$ mAP=\\\\sum_0^N\\\\frac{AP(i)}{N} $$\"}            Given N different models, the optimal model choice in *most*           situations is the one with the highest mAP            F1 score            F1 score is described as the harmonic mean of precision and recall.           If we were to calculate the F1 score for every point on the PR curve,           the point with the highest F1 score is generally chosen as an           operating point.          {\"$$ F1=\\\\frac{2PR}{P+R} $$\"}              This is also known as the Equal Error Rate (EER) point."
  },
  {
    "slug": "efficientnet-v1-v2-a-smart-heuristic",
    "title": "EfficientNet (V1 & V2) - A smart heuristic",
    "excerpt": "",
    "content": "            Intuition            EfficientNet tries to come up with a smart heuristic to scale a CNN,           relating resolution, width, and depth of a CNN. In particular, it           tries to answer two key questions:            What should be the best base network           How to scale the base network(a) in an efficient manner            EfficientNet enables us to effectively control the compute           used(FLOPs) by a network Vs accuracy. Moreover, it allows for fast           inference on embedded devices.            Finding the best base network            The author uses a multi-objective neural architecture search           algorithm to find a network (EfficientNet-B0). The objective function           comprises of{' '}           Accuracy(x) * (Flops(x) / Target-Flops)  as the target.              MB stands for inverted bottleneck residuals from MobileNet v2            Scaling the base network            Now that the author has found the best network to scale, the question           becomes how to relate resolution, width and depth of the network by           one number,making it easier to scale. The technique used in this paper           tries to answer the following question:            How would you scale the network if you suddenly had twice as many           resources ?            By trying different multiples of resolution, width, and depth of the           network, the author lands on the multiples 1.15, 1.1, and 1.2           respectively. This means that if we scale the resolution, width, and           depth of the network by the above-mentioned multiples, we will use           twice as much compute compared to the base case when the multiples           are 1 each. If we apply this approach in an incremental fashion, we           will obtain optimized architectures which give high accuracy at the           specified target flops.            UPDATE: EfficientNet V2          Some changes proposed in the latest architecture and methodology:              Combination of Fused-MBConv and MB Conv instead of only MbConv              Training-aware NAS - jointly optimizes for accuracy, parameter             efficiency and training speed this time.              Progressive Learning - Low regularization + small images initially             during training, followed by high regularization + large images             later.            Small architecture changes.            Results              Results show a remarkable trade-off b/w accuracy and compute             obtained by this heuristic.            References              (                https://arxiv.org/pdf/1905.11946.pdf              ) EfficientNet: Rethinking Model Scaling for CNNs - Mingxing Tan,             Quoc V. Le"
  },
  {
    "slug": "grounded-sam-2",
    "title": "Notes - Grounded SAM",
    "excerpt": "The Grounded SAM paper introduces a novel approach to open-set segmentation by combining two powerful pre-trained models: Grounding DINO for open-set object detection and the Segment Anything Model (SAM) for zero-shot segmentation.",
    "content": " What is Grounded SAM?            The{\" \"}              Grounded SAM           {\" \"}           paper introduces a novel approach to open-set segmentation by           combining two powerful pre-trained models: Grounding DINO for open-set           object detection and the Segment Anything Model (SAM) for zero-shot           segmentation. This integration enables the detection and segmentation           of any regions based on arbitrary text inputs and opens a door to           connecting various vision models.            Model components used in the paper:            SAM is an open-world segmentation model that can \"cut           out\" any object in any image with proper prompts, like points, boxes,           or text. Despite of its strong zero-shot performance, the model cannot           identify the masked objects based an arbitrary text input and normally           requires point or box prompts to run.            DINO is Meta’s versatile foundation model trained           using self supervision. Features from this model can be used in a           variety of downstream tasks like Monocular depth estimation,           segmentation, object detection, etc.            Grounding DINO is an open-set object detector that can           detect any objects with respect to an arbitrary free-form text prompt.           It has a strong zero-shot detection performance. However, the model           needs text as inputs and can only detect boxes with corresponding           phrases.            OSX is the state-of-the-art model for expressive           whole-body mesh recovery, which aims to estimate the 3D human body           poses, hand gestures, and facial expressions jointly from monocular           images. It needs first to detect human boxes, crop and resize the           human boxes, and then conduct single-person mesh recovery.            BLIP is an Image caption model. It cannot perform           object level tasks like detection/segmentation.            Recognize Anything Model (RAM) is a strong image           tagging model that can recognize any common categories of high           accuracy for an input image. However, RAM can only generate tags but           cannot generate precise boxes and masks for the recognized categories.            How does SAM work?            Given an input image and a text prompt, we first employ Grounding DINO           to generate precise boxes for objects or regions within the image by           leveraging the textual information as condition. Subsequently, the           annotated boxes obtained through Grounding DINO serve as the box           prompts for SAM to generate precise mask annotations. By leveraging           the capabilities of these two robust expert models, the open-set           detection and segmentation tasks can be more effortlessly           accomplished.            Why not train a universal object segmentation model with text           prompting instead?            It is highly challenging to determine masks in images corresponding           to regions mentioned in any user-provided text. This is primarily due           to the limited availability of high-quality data for segmentation in           the wild tasks, which presents a challenge for the model to accomplish           precise open-set segmentation under conditions characterized by data           scarcity.            Which new pipelines are enabled with Grounded SAM ?          4 workflows described in this picture are:            G-DINO + SAM = Open vocabulary object detection + segmentation              BLIP & RAM = Automatic detection + segmentation labels from any             image              Grounded SAM + Stable diffusion = Automatic Synthetic data             generation + controllable image editing system              Grounded SAM + OSX = Mesh recovery + human motion analysis"
  },
  {
    "slug": "migrate-your-blog-from-ghost-pro",
    "title": "Migrate your blog from Ghost(Pro) to Digital ocean",
    "excerpt": "",
    "content": " I recently decided to migrate my ghost blog from ghost(pro)           subscription to a digital ocean droplet. Primary reasons for the           migration were:            Inability to modify themes with a basic subscription           Limited integrations.          Here are the steps involved in the migration:             Downloading content from Ghost(pro) website              Navigate to your Ghost dashboard ➡ Settings ➡ Labs                Export your content               Download current redirects               Download current routes.yaml              Navigate to your Ghost dashboard ➡ Settings ➡ Design                  Go to Change theme ➡ Advanced ➡ Download              Navigate to Members ➡ Settings ➡ Export all members              Send an email to{' '}                support@ghost.org             {' '}             from your registered email asking for a zip of all the images used             in your website.             Creating a droplet on Digital Ocean              Create an account and sign in on{' '}                Digital Ocean              Click{' '}                here             {' '}             and then select 'Create Ghost Droplet'              Make sure it says \"Ghost latest on Ubuntu XX.xx\". If it             doesn't say that, you may end up creating a droplet without             Ghost.              The default droplet configuration works fine for a regular ghost             website with a few hundred visitors per day            Add your SSH key              Do not enable Monitoring unless you choose a bigger             droplet ➡ Adding a monitoring agent will make your website             frequently unresponsive, as it is resource intensive.            Create the droplet. It will be ready in a minute!             Configuring Domain              Click on 'Droplets' on the control panel. Now you can see             all your droplets.              Go to the settings dropdown menu for your droplet and select             'Add Domain'. Make sure you're setting the domain for             the droplet and not for the \"Project\".              Type the name of your website (clearsignal.xyz in my case) and click             on 'Add Domain'              It should create 4 entries as shown below              Navigate to your domain name registrar (Namecheap in my case) and             set custom nameservers as mentioned{' '}                here              .             Ghost setup on droplet              Type the following command:{' '}             {'ssh root@{public ip of droplet}'}              Wait till the ghost installation is complete                Enter the relevant information when prompted              Go to {'{domainname}/ghost'} to configure your             website.              Navigate to Settings ➡ Labs ➡ Delete All content              Click on Import content. Don't forget to click on the             'import' button after selecting the file.            Upload redirects and routes              Go to settings ➡ design ➡ upload theme . Make sure to             rename the theme before uploading.              Copy images from the zip emailed to you from ghost support to the             droplet. Use the following command:                {                 'scp -pr {Local path to extracted zip from support@ghost.org}/content/images/  root@{Droplet Public ip}:/var/www/ghost/content/images/*'               }            Now you can modify your ghost theme and upload it anytime.           Subscription costs came down from 11$/month to 6$/month Affine           Augmentations‌"
  },
  {
    "slug": "multi-task-losses",
    "title": "Multi-task Loss",
    "excerpt": "This is a short review of the paper titled Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics by Kendall et al, 2018 .",
    "content": " This is a short review of the paper titled \"Multi-Task           Learning Using Uncertainty to Weigh Losses for Scene Geometry and           Semantics\" by Kendall et al, 2018 .            Problem description            Given a model with multiple heads(tasks), how to balance all the           losses ? The gradients are affected by the magnitude of losses, and           naturally the tasks with higher loss magnitudes are prioritized.            Key Contribution            The author uses the example of a multi-head model predicting           semantic segmentation, instance segmentation and inverse depth in           this paper to demonstrate his experiment.            Core idea              Let each model branch predict its own{\" \"}             homoscedastic uncertainty            Weigh each loss by the branch's respective uncertainty            Proof            Let {\"\\\\( f^W(x) \\\\)\"} be the           output of a neural network with weights{\" \"}           {\"\\\\( W \\\\)\"} on input{\" \"}           {\"\\\\( x \\\\)\"}. For regression           tasks we define our likelihood as a Gaussian with mean given by the           model output :            {\"\\\\( p(y|f^W(x)) = \\\\mathcal{N}(f^W(x),\\\\sigma^{2}) \\\\tag{1} \\\\)\"}            Multi task likelihoods can be defined as (assuming independent           random variables) :            {             \"\\\\(p (y_{1},\\\\dots, y_{k}|f^W(x) ) = p( y_{1}|f^W(x)),\\\\dots,p( y_{k}|f^W(x)) \\\\tag{2} \\\\)\"           }            The probability density of observing a single data point x, that is           generated from a Gaussian distribution is given by:            {             \"\\\\( f(x) = \\\\frac{1}{\\\\sigma \\\\sqrt{2\\\\pi}} e^{\\\\frac{-(x-\\\\mu)^2}{2\\\\sigma^2}} \\\\tag{3} \\\\)\"           }            Substituting (3) in (1), Log likelihood for multi task input can be           defined as:            {             \"\\\\( \\\\log_{e}{p(y|f^{W}(x))} \\\\propto \\\\frac{-1}{2\\\\sigma^2} \\\\lVert y-f^{W}(x)\\\\rVert^2 - \\\\log{\\\\sigma} \\\\tag{4} \\\\)\"           }            Now let's assume that our model output is composed of two           vectors y1 and y2, each following a Gaussian distribution such that:            {             \"\\\\( p(y1,y2|f^{W}(x)) = p(y1|f^{W}(x)) \\\\cdot p(y2|f^{W}(x)) = \\\\mathcal{N}(y1;f^W(x),\\\\sigma_{1}^{2}) \\\\cdot \\\\mathcal{N}(y2;f^W(x),\\\\sigma_{2}^{2}) \\\\tag{5} \\\\)\"           }          Taking log on both sides and expanding (5) using (4), we get:            {             \"\\\\( - \\\\log{p(y1, y2, | f^{W}(x))} \\\\propto \\\\frac{1}{2\\\\sigma_{1}^2} \\\\lVert y_{1}-f^{W}(x)\\\\rVert^2 + \\\\frac{1}{2\\\\sigma_{2}^2} \\\\lVert y_{2}-f^{W}(x)\\\\rVert^2 + \\\\log{\\\\sigma_{1}\\\\sigma_{2}} \\\\)\"           }            {             \"\\\\(   = \\\\frac{1}{2\\\\sigma_{1}^2}\\\\mathcal{L}_{1}(W) + \\\\frac{1}{2\\\\sigma_{2}^2}\\\\mathcal{L}_{2}(W) +  \\\\log{\\\\sigma_{1}\\\\sigma_{2}} \\\\)\"           }            As {\"\\\\( \\\\sigma_{1} \\\\)\"} (the           noise parameter for the variable{\" \"}           {\"\\\\( y_{1} \\\\)\"} ) increases,           weight of{\" \"}           {\"\\\\( \\\\mathcal{L}_{1} \\\\)\"}{\" \"}           decreases. On the other hand, as the noise decreases, the weight of           the respective objective increases. The noise is discouraged from           increasing too much (effectively ignoring the data) by the last           term in the objective, which acts as a regulariser for the noise           terms.              In practice, we train the network to predict the{\" \"}             {\"\\\\( \\\\sigma^{2} \\\\)\"}. This is             numerically more stable than regressing the variance,{\" \"}             {\"\\\\( \\\\sigma \\\\)\"}, as the loss             avoids any division by zero.            Results            A reasonably well trained model is obtained without the need for           manual loss weight adjustment. There's a minor improvement           observed in segmentation + inverse depth metrics."
  },
  {
    "slug": "my-new-desktop",
    "title": "My New Desktop!",
    "excerpt": "",
    "content": " If you're into gaming and deep learning, you need to own a GPU. For           years I was working with an older GPU (GTX 960M), but I thought it was           time to upgrade. But wait, should I get a laptop, a pre-built           desktop, or just build my own ? It took me weeks to decide, and now I           can say that I don't regret my decision in hindsight. I ended up           assembling a desktop from scratch, and I must say, it was a great           experience !             Why build your own desktop?          There are multiple advantages:              Cost Effective                It's way cheaper as compared to buying pre-built desktops.                  There are plenty of options on the market, like HP Omen series,                 CLX custom builds, ROG desktop series, etc. I ended up adding                 better parts and saved approximately 600$ !              Customizable and Upgradable                  If a part falls out, it might not be easy to replace it in a                 pre-built desktop. I've read about such builds having little to                 no space b/w parts.                  Future GPU and CPU upgrades are easier if you've installed the                 parts yourself.                  Normally, people end up adding more fans which prolongs the life                 of CPU and other components.             Part List              Motherboard:{' '}                ASUS ROG Strix B550-F Gaming              PSU :{' '}                EVGA 750 N1, 750W              Ram:{' '}                TEAMGROUP T-Force Vulcan Z DDR4 16GB Kit (2x8GB) 3000MHz              CPU Cooler:{' '}                Vetroo V5 CPU Air              SSD: 2 x{' '}                Crucial P2 500GB 3D NAND NVMe              CPU:{' '}                AMD Ryzen 7 5800X              Case:{' '}                Thermaltake Level 20 MT              GPU:{' '}                Gigabyte GeForce RTX 2060 6GB GDDR6 Graphics Card            Notes              I would recommend replacing the CPU cooler with another one, since             the mounts on this one didn't fit the AM4 socket on the motherboard.              Currently, there is a massive chip shortage due to covid and other             supply chain issues in China. GPUs are hard to come by. I would             recommend buying a used GPU (make sure it wasn't used for crypto             mining), or asking a friend who works at Nvidia to get you one.              I found the following websites to be extremely useful:                    PC Part Picker                    Logical increments"
  },
  {
    "slug": "poly-loss",
    "title": "PolyLoss : A new framework for loss functions",
    "excerpt": "",
    "content": " Original Paper: [              https://arxiv.org/pdf/2204.12511.pdf            ]            Authors: Zhaoqi Leng, Mingxing Tan , Chenxi Liu , Ekin Dogus Cubuk ,           Xiaojie Shi, Shuyang Cheng ,Dragomir Anguelov [Waymo And Google]             Summary            Authors propose a new framework of loss functions, motivated by the           Taylor series expansion of commonly used functions like cross entropy           and focal loss.            When CE and focal losses are expanded, it's easier to see the           similarities and differences between them. This seems to be the prime           motivation to create a framework of losses which express such common           functions as a special case.            The authors also experiment with the coefficients of taylor expansion           of these functions and assess their impact on training a ResNet-50           model on ImageNet-1k dataset. Based on their experiments, altering           the coefficients of the first N terms of the expansion series seems           to give the best results.            Upon further simplification, L(poly-1) seems to provide superior           result as compared to cross entropy loss on a variety of different           tasks involving detection, classification and segmentation.            There's a lot to love about the simplicity and impact of this paper.           A one line change in the loss function seems to give better results           than the traditionally used loss functions."
  },
  {
    "slug": "polynomial-fitting-using-pytorch",
    "title": "Polynomial fitting using Pytorch",
    "excerpt": "Here is a short tutorial on how to fit polynomials using pytorch.",
    "content": " Here is a short tutorial on how to fit polynomials using pytorch.            NOTE: This was written a few years ago. I found this notebook burried in           my SSD yesterday and thought I should share it.             Fitting Polynomials          Creating a polynomial module of second order a+bx+cx^2=0          Generate Parabolic data          Training loop          Optimizer variables  ```python import torch device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  class Polynomial_2(torch.nn.Module):     def __init__(self) -> None:         super().__init__()         self.a = torch.nn.Parameter(torch.randn(()))         self.b = torch.nn.Parameter(torch.randn(()))         self.c = torch.nn.Parameter(torch.randn(()))      def forward(self, x):         return self.a + self.b * x + self.c * torch.pow(x, 2)  import numpy as np  def generate_parabolic_data(N, a, b, c):     \"\"\"Genrates parabolic data around a curve a+bx+cx^2=0\"\"\"     x = np.linspace(-10, 10, N)     y = a + b * x + c * x**2     return x.astype(np.float32), y.astype(np.float32)  X, Y = generate_parabolic_data(10, 1, 2, 3)  num_rows = 1000 lr = 1.e-1 num_epochs = 25 batch_size = 16 num_batches = num_rows // batch_size  X, Y = generate_parabolic_data(num_rows, 1, 2, 3) X = torch.from_numpy(X).to(device) Y = torch.from_numpy(Y).to(device)  polynomial_model = Polynomial_2() polynomial_model.to(device)  optim = torch.optim.Adam(polynomial_model.parameters(), lr=lr)  for epoch in range(num_epochs):     for batch_idx in range(num_batches):         optim.zero_grad()         y_hat = polynomial_model.forward(X[batch_idx * batch_size:(batch_idx + 1) * batch_size])         loss = torch.nn.functional.mse_loss(y_hat, Y[batch_idx * batch_size:(batch_idx + 1) * batch_size], reduction='sum')         loss.backward()         optim.step()     if (epoch % 5 == 0 or epoch == num_epochs - 1):         print(f'epoch: {epoch} | loss: {loss.item():0.3f}')  print(f\"\\\\nInput Data equation is generated for the equation: 1 + 2x + 3x^2\") print(f\"Equation after optimizing is: {polynomial_model.a.item():.4f} + {polynomial_model.b.item():.4f}x + {polynomial_model.c.item():.4f}x^2\")  epoch: 0 | loss: 3889.375 epoch: 5 | loss: 6.941 epoch: 10 | loss: 0.812 epoch: 15 | loss: 0.029 epoch: 20 | loss: 0.002 epoch: 24 | loss: 0.001  Input Data equation is generated for the equation: 1 + 2x + 3x^2 Equation after optimizing is: 1.0061 + 1.9993x + 3.0000x^2  for param_tensor in optim.state_dict():     print(f\"\\t {param_tensor}: {optim.state_dict()[param_tensor]}\")  \t state: {0: {'step': tensor(1550.), 'exp_avg': tensor(0.0296, device='cuda:0'), 'exp_avg_sq': tensor(28273.8398, device='cuda:0')}, 1: {'step': tensor(1550.), 'exp_avg': tensor(0.6155, device='cuda:0'), 'exp_avg_sq': tensor(1985039.1250, device='cuda:0')}, 2: {'step': tensor(1550.), 'exp_avg': tensor(8.0943, device='cuda:0'), 'exp_avg_sq': tensor(1.5650e+08, device='cuda:0')}} \t param_groups: [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2]}]  "
  },
  {
    "slug": "quick-nlp-overview",
    "title": "Short NLP Overview!",
    "excerpt": "",
    "content": "            Outline                Introduction                Neural Network Modeling                Available resources                References             Introduction            Natural language processing (NLP) is a branch of           science sitting at the intersection of computer science, artificial           intelligence, and computational linguistics. To put it simply, it           gives computers the ability to read, understand and generate text and           speech just like we do. Within the last decade, NLP-based products           have become ubiquitous. Here are some common            NLP applications/products you might be aware of:            Amazon Alexa or Siri           Voice input in Google or Apple Maps           Conversational chatbots           Google or Siri Translate            From a computer vision scientist's perspective, NLP is becoming           increasingly important as time goes by. From the use of pre-trained           embeddings to vision transformers, we are starting to observe how           architectures used in both fields have concepts in common. For all           aspiring AI practitioners, it is important to know the basic           algorithms in            both fields.              Source: Amazon          NLP can be broadly broken down into 4 key areas:                Language Modeling              Language modeling is the task of assigning a probability to             sentences in a language. Besides assigning a probability to each             sequence of words, the language model also assigns a probability for             the likelihood of a given              word (or a sequence of words) to follow another sequence of words.             [1]                Morphology              Morphology is the study of words, how they are formed, and their             relationship to other words in the same language. It analyzes the             structure of words and parts of words such as stems, root words,             prefixes, and suffixes. [2]                Parsing / Syntactic Analysis              Syntactic analysis, also referred to as syntax analysis or parsing,             is the process of analyzing natural language with the rules of             formal grammar. Grammatical rules are applied to categories and             groups of words, not individual words, thereby assigning a semantic             structure to text. [3]                Semantic Analysis              Semantic analysis refers to the process of understanding the meaning             behind sentences. it is a study of what words mean when they are put             together in a sentence. It takes into account the individual meaning             of words, how they relate and modifies neighboring words, and the             context these words appear in.             Neural Network Modeling            Modern-day NLP mostly relies on deep neural networks(DNN) for           predictions. NLP engineers have to make 3 important choices before           they train any neural network:            Defining the \"Task\"           Choosing word embeddings           Choosing the neural network architecture            Defining the \"Task\"            Here are some broad categories defined in the AllenNLP model library.           Code + paper + weights are available at{' '}              Papers with Code           {' '}           website.              Source: Papers with Code            a.{' '}              Classification            : Predicting one or more labels for each input. For example Sentiment           analysis.            b.{' '}              Coreference Resolution            : The goal is to find all expressions that are related to the same           entity.              Example of Coreference Resolution. Source: Stanford NLP Group            c.{' '}              Generation            : Generating unstructured text of variable length. For example{' '}              GPT-3            .            d.{' '}              Language Modeling            : Learning a probability distribution over a sequence of tokens.            e.{' '}              Sequence Tagging            : Information extraction task where each word in a sentence is           classified with a pre-defined label set.              Example of Sequence Tagging. Click image for source.              PS:           {' '}           This is not an exhaustive list.            Choosing word embeddings            NLP deals with massive vocabularies, ranging from 100k to millions of           words. It would be inefficient to train a network if each word is           represented with a vector of size V (where V is the size of this           vocabulary). Word embeddings solve this problem as they reduce the           dimensions of the encoding (compared to V), while adding meaning to           the vectors.            Here are some common word embeddings:                Embedding Layer              This is a neural network layer with weight dimensions (V x N), where             V is the size of the vocabulary, and N is the embedding dimension.             It is initialized with random values and the word embeddings are             learned during the training phase. [                Detailed Explanation              ]                Word2Vec              Word2Vec was developed by Mikolov et al.{' '}                paper             {' '}             in 2013. This technique uses the              local context around a word to develop a meaningful representation             of it. It has two flavors:                Continuous Bag Of Words Model                  Skip-gram Model                  More details can be found{' '}                    here                  .                GloVe (Global Vectors for Word Representation)              GloVe was developed by Manning et al. At Stanford University. The             main intuition underlying the model is the simple observation that             ratios of word-word co-occurrence probabilities have the potential             for encoding some form of meaning.              More details can be found{' '}                here              .                ELMO (Embeddings from Language Models)              All the embeddings discussed above aim to provide 1 embedding per             word, but we know that the meaning of a word changes with context.              To solve this problem, ELMO trains custom deep{' '}             embeddings for each word, based on the context/usage. For example,             the word \"right\"              will have different embeddings for the following sentences:                Take the next right.               You are right about that problem.                  BERT (Bidirectional Encoder Representations from Transformers)              Both Elmo and Bert are deep embeddings that involve training             wider/deeper neural networks on a big corpus.              The key difference between the two lies in the training objective.                  ELMO uses the traditional Language Modeling training objective                 to generate the next word given previous words in the sentence.                 It tries to calculate P(Xi| Xi-1, Xi-2, Xi-n) such that the                 resulting distribution mimics the distribution in the training                 corpus.                  Bert uses the 'masked' language modeling objective. It                 randomly masks words (with some probability) in the training                 corpus and uses the surrounding words to predict the missing                 word.            Choosing the neural network architecture          There are two classes of architectures we should look at:            Traditional Sequence models          The most commonly used sequence models are :            a.{' '}              RNN (Recurrent neural networks)            Recurrent neural networks are a class of networks that consume           sequential inputs. They update the current state(memory) based on the           previous state and the current input.              Source            b.{' '}              LSTM (Long short term memory) networks            LSTM is a type of RNN that gets rid of the vanishing gradient problem           by regulating the gradient flow using gates.              Source            c.{' '}              GRU (Gated recurrent unit) networks            GRU is a simplified version of LSTM, with only 2 gates instead of 3.           It's faster to train as compared to LSTMs and provides similar           performance metrics on a variety of datasets.            NOTE: A step by step explanation on these models can be found at{' '}              Colah's Blog            .            Transformers            Transformer networks are one of the most widely used architectures in           NLP over the last 4 years. The architecture deploys a novel use of the           familiar concepts of{' '}              self-attention            ,{' '}              multi-head attention            , and{' '}              positional encodings           {' '}           inside an encoder-decoder design, thereby enhancing the performance of           the model.{' '}                GPT-3 uses a modified version of this architecture and almost               passed the Turing test for short articles!              Source            Please refer to this{' '}              blog           {' '}           for more details on transformers.             Available Resources            Paid APIs for NLP            The API industry for NLP has really blossomed over the years. You can           find APIs for all the tasks listed{' '}              here            .            Here are a few APIs that I came across:            Google Cloud           HuggingFace           NLP Cloud           IBM Watson           Aylien            Python Libraries for NLP            I've been experimenting with AllenNLP, which provides pre-built           models, encoder/decoder blocks, vocabulary builders, indexers, and           tokenizers. It's a layer on top of Pytorch and seems pretty           intuitive. Moreover, the documentation provided is sufficient to get           started with, which isn't the case with TorchText as of now.           Other than that, they don't differ a lot.            Here are some open-source python libraries which can be used for model           training and inference:            TorchText : Torch Library           Spacy : This is equivalent to 'scipy' for NLP !              HuggingFace: primarily used for transformers, sharing open-source             models, datasets and metrics            AllenNLP: research framework built on pytorch            This concludes this short intro to NLP. I'll try to snipe at           certain topics and provide a detailed analysis in the upcoming           articles.            Thanks for reading!             References              Page 105,{' '}                Neural Network Methods in Natural Language Processing, 2017.                Wiki                Introduction to NLP                A Survey of the Usages of Deep Learning for Natural Language               Processing                Allen Institute for AI                The Illustrated BERT, ELMo, and co."
  },
  {
    "slug": "retrieval-augmented-generation-notes",
    "title": "NOTES: Retrieval-Augmented Generation for Large Language Models",
    "excerpt": "",
    "content": " Here is a short compilation of bullet points gathered while reading           the paper \" Retrieval-Augmented Generation for Large Language           Models: A Survey\". The authors did a reasonably good job           summarizing the landscape of current RAG systems.            Paper:{' '}              Retrieval-Augmented Generation for Large Language Models: A Survey            Authors: Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu           Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wang             Notes              Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving             relevant document chunks from an external knowledge base through             semantic similarity calculation.              Development techniques:                Pretraining LLMs               Inference/ In-context learning               Finetuning LLMs              Evolution of RAG:                Naive               Advanced               Modular              Naive RAG                  3 steps                    Indexing: Documents → chunks → vectors → Vector DBs                   Retrieval : Query → Vector → Semantic similar search → top K                   Generation : Query + top K vectors → Generated response                  Cons:                    Imprecise retrievals: Irrelevant chunks                      Too many similar chunks are retrieved leading to repeated                     responses              Advanced RAG                  Pre-retrieval and post-retrieval strategies                      Pre-retrieval                          Enhancing data granularity, optimizing index                         structures, adding metadata, alignment optimization, and                         mixed retrieval.                        Query rewriting, query transformation                      Post-retrieval                          Feeding all relevant chunks can dilute key details with                         irrelevant content                        Reranking + context compression helps here                          Reranking retrieved information to the edges of the                         prompt is the key              Modular RAG                  Multiple components more flexibility and adaptability as modules                 can be substituted depending upon task                Numerous different patterns/strategies                  Types of modules:                    Search                   Fusion                   Memory                   Routing                   Predict                   Task adapter                  Types of strategies:                    Rewrite-Retrieve-Read                   Generate-Read                   Recite-Read              RAG vs Fine-tuning                  RAG excels in dynamic environments by offering realtime                 knowledge updates and effective utilization of external                 knowledge sources with high interpretability. However, it comes                 with higher latency and ethical considerations regarding data                 retrieval.                  FT is more static, requiring retraining for updates but                 enabling deep customization of the model’s behavior and style.                 It demands significant computational resources for dataset                 preparation and training, and while it can reduce                 hallucinations, it may face challenges with unfamiliar data.                  While unsupervised fine-tuning shows some improvement, RAG                 consistently outperforms it, for both existing knowledge                 encountered during training and entirely new knowledge.                  LLMs struggle to learn new factual information through                 unsupervised fine-tuning, making a case for RAG             Summary              Ecosystem             Simple RAG Pipeline using Langchain          {codeSnippet1}         {output1}            Load blog          {codeSnippet2}            Setup vector Store          {codeSnippet3}            Setup Retreiver          {codeSnippet4}         {output2}            Setup Prompt and chain          {codeSnippet5}         {output3}            Invoke Prompt          {codeSnippet6}         {output4}"
  },
  {
    "slug": "semantic-segmentation-deeplab-v3-2",
    "title": "Semantic Segmentation - DeepLab V3+",
    "excerpt": "",
    "content": "            Semantic Segmentation            Semantic segmentation involves partitioning/marking regions in the           image belonging to different objects/classes. Deep learning methods           have made a remarkable improvement in this field within the past few           years. This short article summarises DeepLab V3+, an elegant           extension of DeepLab v3 proposed by the same authors (Chen et al.).              Examples of semantic segmentation.{' '}                Source             Intuition              (a) ASPP style architecture (b) Encoder Decoder style architecture             (c) Proposed architecture.{' '}                Source            Previously, ASPP (Atrous Spatial Pyramid Pooling) has been used to           extract rich multi-scale features from images. The authors of Deeplab           v3+ try to combine the ASPP module with the good old encoder-decoder           architecture with skip connections, thereby providing better details           in predictions.             Architecture              DeepLab v3+ architecture.{' '}                Source          Here are the key features of this architecture:              Atrous Depthwise Convolution: The depthwise convolution has an added             dilation to make it atrous.              ASPP style encoder from DeepLab V3 + UNet style decoder with skip             connections.              Modified Xception network as the backbone: This can be replaced by             any backbone; HRNet seems to be widely used these days.             Results              Results show different backbones with Bilinear Upsampling(BU) vs a             decoder.{' '}                Source             References                Encoder-Decoder with Atrous Separable Convolution for Semantic               Image Segmentation                Xception: Deep Learning with Depthwise Separable Convolutions                DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,               Atrous Convolution, and Fully Connected CRFs                MobileNetV2: Inverted Residuals and Linear Bottlenecks"
  },
  {
    "slug": "svd",
    "title": "SVD",
    "excerpt": "",
    "content": "            Derivation            In simple words, any matrix 'A' can be decomposed into a product of 3           matrices : U, ∑ and V, where ∑ contains the singular values along its           diagonal, and U & V are unitary matrices. Here is a derivation for           SVD:             Applications            Image compression           Low-rank approximations           PCA           Rank determination           Least squares          and a million more...         Let's look at one such application in computer vision:            Image compression            What if we delete small singular values from ∑ and its corresponding           vectors from U and V? We can then obtain the projection of A onto a           lower-dimensional subspace. This technique can be used to compress an           image at the loss of some high-frequency information.            Using this code, we can reconstruct the image using the first N           components:          {pythonCode}            There are 942 singular values for the original image, since it's a           (942x942) image with unique pixel rows.            We can recover most of the low-frequency information from the image           using only the first 15-30 components of SVD. Even the finer details           like eyeballs are present in the reconstructed image, with a           compression rate of ~ 2000-4000."
  },
  {
    "slug": "yolo-x-notes",
    "title": "Yolo-X Notes",
    "excerpt": "The main motivation behind YOLOX was to update the YOLO series with the recent advancements at the time, particularly anchor-free detection.",
    "content": " Motivation              The main motivation behind YOLOX was to update the YOLO series with             the recent advancements at the time, particularly anchor-free             detection.              When YoloX came out in 2021, YOLOv5 held the best trade-off             performance with 48.2% AP on COCO at 13.7 ms. This inference time             was calculated using YOLOv5-L model at 640 × 640 resolution with             FP16-precision and batch=1 on a V100.              YOLOv4 and YOLOv5 still used anchor-based detectors with             hand-crafted assigning rules for training.              YOLOX comparison with its peers at the time (Aug 2021)            Novelty              This paper chose Yolov3 as the baseline and added incremental             improvements to it, such as:              Anchor free detection                  Eliminates the need for clustering analysis to determine optimal                 anchor sizes. Such anchors are domain-specific and less                 generalized.                  Anchors increase complexity in two ways. Firstly, the number of                 predictions from the detection heads is too high. This can cause                 potential memory bottlenecks. Secondly, anchors are associated                 with several tightly tuned design parameters, making decoding                 harder.                The anchor-free concept appears to have been adapted from the{\" \"}                  FCOS               {\" \"}               paper.              SimOTA label assignment                YoloX adopts a simplified version of the label assignment strategy               created by the same authors in the{\" \"}                  OTA               {\" \"}               paper. Labels are matched to the predictions based on a cost               value (weighted sum of losses). Then an IOU is calculated relative               to the sampled center (center-sampling from{\" \"}                  FCOS                ) to determine the top k matches, also known as the Dynamic-k               strategy.                Strong augmentation - Mosaic and Mixup                Strong augmentation ensures that pre-training with Imagenet is no               longer beneficial. Models are trained from scratch with this               augmentation.                Decoupled head for classification and regression                Improves convergence speed               Improves AP            Results              Baseline: Beats YoloV3 baseline by{\" \"}             3% mAP on Coco when using the same backbone             (DarkNet53)              Large model (YOLOX-L): Beats YoloV5-L baseline by{\" \"}             1.8% mAP on Coco when using the same backbone and             other enhancements (CSPNet and additional Pan head)              Small model (YOLOX-Tiny): Beats YoloV4-Tiny             baseline by 10% mAP on Coco when using the same             backbone and other enhancements            Links              Paper           {\" \"}           /{\" \"}              Github            References                FCOS: Fully Convolutional One-Stage Object Detection                OTA: Optimal Transport Assignment for Object Detection                YOLOX: Exceeding YOLO Series in 2021"
  }
]